# README.md

## Описание домашней работы

Данная домашняя работа представляет собой задачу по улучшению модели машинного обучения, обучаемой на тексте книги "Гарри Поттер и Орден Феникса" на русском языке. Цель — разработка и оптимизация модели на основе трансформеров для токенизации текста, обучения и генерации текста. Работа включает создание модульной структуры кода и улучшение качества генерации.

## Файлы и структура домашней работы

- `tokenizer.json` - конфигурация токенизатора с поддержкой Byte-Level BPE.
- `data.py` - модуль для обработки данных.
- `model.py` - определение модели трансформера.
- `train.py` - скрипт для обучения модели.
- `generate.py` - скрипт для генерации текста.
- `results` - папка с результатами
- `text.txt` - Гарри Поттер 

## Пример выполнения

### Обучение модели
Запуститив `train.py`:
```
PS C:\Users\79022\Desktop\SummerParty\homework6> & C:/Users/79022/Desktop/SummerParty/homework6/env/Scripts/python.exe c:/Users/79022/Desktop/SummerParty/homework6/train.py
[00:00:00] Предобработка файлов (0 Мо)    ██████████████████████████████████████████████████████████████████                100%
[00:00:00] Токенизация слов               ██████████████████████████████████████████████████████████████████ 15166    /    15166
[00:00:00] Подсчет пар                    ██████████████████████████████████████████████████████████████████ 15166    /    15166
[00:00:00] Вычисление слияний             ██████████████████████████████████████████████████████████████████ 24444    /    24444
GPU доступен: True (cuda), используется: True
...
Epoch 2: 100%|████████████████████████████████████████████████████| 472/472 [00:45<00:00, 10.34it/s, v_num=4, train_loss=6.320]
`Trainer.fit` остановлен: достигнуто `max_epochs=3`.
```

### Генерация текста
Запустив `generate.py`:
```
PS C:\Users\79022\Desktop\SummerParty\homework6> & C:/Users/79022/Desktop/SummerParty/homework6/env/Scripts/python.exe c:/Users/79022/Desktop/SummerParty/homework6/generate.py
Введите 'quit' для выхода.
Вы: Гарри Поттер?
Бот (обычная генерация): Гарри Поттер? ,? диковатого,, Нимбус для Снэйп лакричные:, часто футбольный
 Гарри И что, над:: в были под палочку попал чудовищного Петуния. всеуслышание
Бот (beam search): Гарри Поттер?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Вы:
```
## Вывод

После выполнения домашней работы установлено, что моя текущая модель демонстрирует сомнительную способность к генерации осмысленных текстов. Обычная генерация дает результат с лишними символами ("Гарри Поттер? ,? диковатого,,..."), а beam search полностью вырождается в повторяющиеся символы. Это указывает на необходимость доработки токенизатора, увеличения числа эпох обучения и оптимизации гиперпараметров модели. Дальнейшая работа должна сосредоточиться на постобработке текста и тестировании с более длинными запросами.
