# Документация к домашнему заданию по полносвязным сетям

## Описание проекта
Этот проект представляет собой серию экспериментов с полносвязными нейронными сетями на датасете MNIST. Целью является исследование влияния глубины сети, ширины слоёв и различных методов регуляризации на точность и время обучения.

## Структура проекта
- `homework_depth_experiments.py` - Скрипт для экспериментов с глубиной сети.
- `homework_width_experiments.py` - Скрипт для экспериментов с шириной слоёв.
- `homework_regularization_experiments.py` - Скрипт для экспериментов с регуляризацией.
- `utils/` - Папка с утилитами:
  - `experiment_utils.py` - Функции для обучения и оценки модели.
  - `visualization_utils.py` - Функции для визуализации результатов.
  - `model_utils.py` - Определение архитектуры сети.
- `results/` - Папка для сохранения текстовых результатов экспериментов.
- `plots/` - Папка для сохранения графиков.
- `data/` - Папка с данными MNIST (локально загруженные файлы).

## Установка зависимостей
Установите необходимые библиотеки:
```
pip install torchvision==0.15.2 matplotlib==3.7.1 seaborn==0.12.2 numpy==1.24.3
```
Убедитесь, что PyTorch уже установлен.

## Результаты экспериментов

### Эксперименты с глубиной сети
| Модель      | Точность на тренировке | Точность на тесте | Время обучения (сек) |
|-------------|-----------------------|-------------------|----------------------|
| 1_layer     | 0.9292               | 0.9174            | 290.29              |
| 2_layers    | 0.9969               | 0.9779            | 297.68              |
| 3_layers    | 0.9944               | 0.9763            | 314.41              |
| 5_layers    | 0.9968               | 0.9797            | 330.40              |
| 7_layers    | 0.9928               | 0.9722            | 408.38              |

**Вывод**: Оптимальная глубина — 5 слоёв, так как она обеспечивает наилучшую точность на тесте (0.9797) при разумном времени обучения.

### Эксперименты с шириной сети

#### 2.1 Сравнение моделей разной ширины
| Модель      | Точность на тренировке | Точность на тесте | Количество параметров | Время обучения (сек) |
|-------------|-----------------------|-------------------|-----------------------|----------------------|
| narrow      | 0.9878               | 0.9714            | 53018                 | 164.26              |
| medium      | 0.9899               | 0.9734            | 242762                | 160.36              |
| wide        | 0.9960               | 0.9822            | 1462538               | 184.86              |
| very_wide   | 0.9917               | 0.9777            | 4235786               | 180.45              |

**Вывод**: Оптимальная модель — wide, обеспечивающая наилучшую точность на тесте (0.9822) при умеренном числе параметров.

#### 2.2 Оптимизация архитектуры
| Модель             | Точность на тренировке | Точность на тесте | Количество параметров | Время обучения (сек) |
|--------------------|-----------------------|-------------------|-----------------------|----------------------|
| expanding_64       | 0.9912               | 0.9758            | 94154                 | 153.76              |
| expanding_128      | 0.9919               | 0.9766            | 270218                | 166.83              |
| expanding_256      | 0.9921               | 0.9786            | 868106                | 172.25              |
| expanding_512      | 0.9921               | 0.9786            | 3046922               | 168.10              |
| expanding_1024     | 0.9920               | 0.9787            | 11336714              | 222.40              |
| constant_64        | 0.9892               | 0.9739            | 59210                 | 150.57              |
| constant_128       | 0.9933               | 0.9776            | 134794                | 152.07              |
| constant_256       | 0.9937               | 0.9777            | 335114                | 172.66              |
| constant_512       | 0.9936               | 0.9775            | 932362                | 169.79              |
| constant_1024      | 0.9931               | 0.9780            | 2913290               | 163.24              |
| narrowing_64       | 0.9942               | 0.9786            | 242762                | 160.05              |
| narrowing_128      | 0.9941               | 0.9799            | 567434                | 196.63              |
| narrowing_256      | 0.9929               | 0.9789            | 1462538               | 184.48              |
| narrowing_512      | 0.9915               | 0.9782            | 4235786               | 178.49              |
| narrowing_1024     | 0.9945               | 0.9802            | 13714442              | 241.64              |

**Вывод**: Оптимальная архитектура — narrowing_1024 с тестовой точностью 0.9802, хотя она требует значительного числа параметров и времени обучения. Баланс между точностью и эффективностью достигается с narrowing_128 (0.9799).

### Эксперименты с регуляризацией
| Регуляризация     | Точность на тренировке | Точность на тесте | Время обучения (сек) |
|-------------------|-----------------------|-------------------|----------------------|
| no_reg            | 0.9939               | 0.9770            | 164.64              |
| dropout_0.1       | 0.9953               | 0.9798            | 158.58              |
| dropout_0.3       | 0.9922               | 0.9813            | 158.76              |
| dropout_0.5       | 0.9841               | 0.9744            | 157.40              |
| batch_norm        | 0.9975               | 0.9811            | 162.25              |
| dropout_batch_norm| 0.9956               | 0.9840            | 161.83              |
| l2_reg            | 0.9914               | 0.9769            | 159.37              |

**Архитектура**: [784, 256, 128, 10]  
**Вывод**: Наилучшая техника — dropout_batch_norm, обеспечивающая максимальную точность на тесте (0.9840) при хорошей стабильности.

### Адаптивная регуляризация
| Модель                | Точность на тренировке | Точность на тесте | Время обучения (сек) |
|-----------------------|-----------------------|-------------------|----------------------|
| adaptive_dropout_0.1  | 0.9942               | 0.9798            | 157.85              |
| adaptive_dropout_0.3  | 0.9926               | 0.9800            | 157.80              |
| adaptive_dropout_0.5  | 0.9858               | 0.9764            | 157.19              |

**Вывод**: Наилучший результат среди адаптивных техник — adaptive_dropout_0.3 с точностью 0.9800, хотя он уступает фиксированному dropout_batch_norm (0.9840).
