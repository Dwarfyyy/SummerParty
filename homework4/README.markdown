# SummerParty Homework 4: Deep Learning Experiments

Этот проект содержит решения для заданий по глубокому обучению, включая сравнение полносвязных и сверточных нейронных сетей, анализ архитектур CNN и эксперименты с кастомными слоями.

## Структура проекта

```
homework/
├── homework_cnn_vs_fc_comparison.py        # Задание 1: Сравнение FC и CNN
├── homework_cnn_architecture_analysis.py    # Задание 2: Анализ архитектур CNN
├── homework_custom_layers_experiments.py    # Задание 3: Кастомные слои и Residual блоки
├── models/
│   ├── fc_models.py                        # Полносвязные модели
│   ├── cnn_models.py                       # Сверточные модели
│   ├── custom_layers.py                    # Кастомные слои
├── utils/
│   ├── comparison_utils.py                 # Утилиты для сравнения
│   ├── visualization_utils.py              # Утилиты для визуализации
│   ├── training_utils.py                   # Утилиты для обучения
├── results/
│   ├── mnist_comparison/                   # Результаты для MNIST
│   ├── cifar_comparison/                   # Результаты для CIFAR-10
│   ├── architecture_analysis/               # Результаты анализа архитектур
│   ├── custom_layers_experiments/           # Результаты кастомных слоев
├── plots/                                  # Все графики по заданиям
├── data/                                   # Данные датасетов
└── README.md                               # Этот файл
```

## Результаты задания 1: Сравнение полносвязной сети и CNN

Результаты производительности моделей на датасете (предположительно CIFAR-10):

| Модель             | Точность на тесте (%) | Время обучения (с) | Время инференса (с) | Количество параметров |
|--------------------|-----------------------|--------------------|---------------------|----------------------|
| Полносвязная сеть  | 97.84                 | 172.02             | 1.8724              | 567,434              |
| Простая CNN        | 99.14                 | 196.49             | 2.1246              | 1,625,866            |
| CNN с Residual     | 98.98                 | 427.19             | 3.2769              | 1,719,242            |

### Анализ
- **Полносвязная сеть**: Меньше параметров, но ниже точность по сравнению с CNN. Быстрее в обучении и инференсе.
- **Простая CNN**: Высшая точность (99.14%), но больше параметров и дольше обучение.
- **CNN с Residual**: Хорошая точность (98.98%), но значительно больше времени на обучение из-за сложной архитектуры.


## Результаты задания 2: Анализ архитектур CNN

Результаты производительности моделей на датасете CIFAR-10 для различных архитектур CNN:

### Влияние глубины сети

| Модель                  | Точность на тесте (%) | Время обучения (с) | Количество параметров |
|-------------------------|-----------------------|--------------------|----------------------|
| 2 слоя без Residual     | 68.55                 | 216.91             | 1,060,266            |
| 4 слоя без Residual     | 77.83                 | 259.81             | 591,658              |
| 2 слоя с Residual       | 68.51                 | 208.39             | 1,060,266            |
| 4 слоя с Residual       | 77.51                 | 258.46             | 630,954              |

#### Анализ
- **2 слоя без Residual**: Умеренная точность (68.55%), быстрее обучение, но больше параметров, чем у 4 слоев без Residual.
- **4 слоя без Residual**: Значительно выше точность (77.83%), но меньше параметров за счет архитектуры.
- **2 слоя с Residual**: Точность аналогична модели без Residual (68.51%), но быстрее обучение.
- **4 слоя с Residual**: Точность близка к модели без Residual (77.51%), но больше параметров из-за shortcut-соединений.

### Влияние размера ядра свертки

| Модель         | Точность на тесте (%) | Время обучения (с) | Количество параметров |
|----------------|-----------------------|--------------------|----------------------|
| Ядра 3x3       | 68.97                 | 246.51             | 2,118,154            |
| Ядра 5x5       | 68.53                 | 265.91             | 2,152,458            |
| Ядра 7x7       | 69.05                 | 321.16             | 2,203,914            |

#### Анализ
- **Ядра 3x3**: Хорошая точность (68.97%) и наименьшее время обучения среди моделей с разными ядрами.
- **Ядра 5x5**: Чуть ниже точность (68.53%), больше параметров и времени обучения.
- **Ядра 7x7**: Наивысшая точность (69.05%), но значительно больше параметров и времени обучения из-за увеличенного размера ядер.

## Результаты задания 3: Кастомные слои и Residual блоки

Результаты производительности моделей на датасете CIFAR-10 для кастомных слоев и различных Residual блоков:

### Кастомные слои

| Модель                  | Точность на тесте (%) | Время обучения (с) | Количество параметров | Стабильность (дисперсия потерь) |
|-------------------------|-----------------------|--------------------|----------------------|---------------------------------|
| Базовая CNN             | 64.69                 | 195.06             | 2,100,490            | 0.1632                          |
| Кастомная свертка       | 63.37                 | 187.60             | 2,100,491            | 0.0609                          |
| Attention               | 64.86                 | 229.12             | 2,105,691            | 0.1486                          |
| Кастомная активация     | 64.88                 | 186.23             | 2,100,491            | 0.1082                          |
| Кастомный пуллинг       | 63.92                 | 188.81             | 2,100,490            | 0.1786                          |

#### Анализ
- **Базовая CNN**: Базовая точность (64.69%), умеренное время обучения, высокая дисперсия потерь (0.1632).
- **Кастомная свертка**: Чуть ниже точность (63.37%), но лучшая стабильность (дисперсия 0.0609) и быстрее обучение.
- **Attention**: Сравнимая с базовой точность (64.86%), но больше параметров и времени обучения из-за механизма внимания.
- **Кастомная активация**: Сравнимая точность (64.88%), быстрее обучение и умеренная стабильность.
- **Кастомный пуллинг**: Точность ниже базовой (63.92%), но стабильность хуже (дисперсия 0.1786).

### Residual блоки

| Модель                  | Точность на тесте (%) | Время обучения (с) | Количество параметров | Стабильность (дисперсия потерь) |
|-------------------------|-----------------------|--------------------|----------------------|---------------------------------|
| Базовый Residual        | 72.83                 | 503.78             | 2,248,714            | 0.0095                          |
| Bottleneck Residual     | 69.47                 | 353.46             | 2,109,770            | 0.0080                          |
| Wide Residual           | 74.38                 | 660.35             | 2,396,554            | 0.0140                          |

#### Анализ
- **Базовый Residual**: Высокая точность (72.83%) и хорошая стабильность (дисперсия 0.0095), но долгое обучение.
- **Bottleneck Residual**: Ниже точность (69.47%), но меньше параметров и быстрее обучение, лучшая стабильность (дисперсия 0.0080).
- **Wide Residual**: Наивысшая точность (74.38%), но больше всего параметров и времени обучения, умеренная стабильность.


### Примечание
Дополнительные эксперименты с кастомными слоями (с меньшим количеством параметров, около 1,050,954) показали схожие тенденции, но с чуть меньшей точностью (62.91–65.37%). Результаты доступны в `results/custom_layers_experiments/`.

## Вывод
- **Задание 1**: Простая CNN превосходит полносвязную сеть по точности (99.14% против 97.84%), но требует больше ресурсов. CNN с Residual улучшает стабильность за счет увеличенного времени обучения.
- **Задание 2**: Глубина сети (4 слоя) повышает точность до 77.83%, а ядра 7x7 дают максимум 69.05%, но с ростом вычислительных затрат.
- **Задание 3**: Кастомные слои, такие как кастомная свертка, улучшают стабильность, но снижают точность. Wide Residual достигает наилучшей точности (74.38%), но с высокой стоимостью.